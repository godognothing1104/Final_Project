{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-04T22:34:58.792385Z",
     "start_time": "2024-12-04T22:34:58.788383Z"
    }
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    " # Randomly initialises values of thetas between [-epsilon, +epsilon]\n",
    "def initialise(a, b):\n",
    "    epsilon = 0.15\n",
    "    c = np.random.rand(a, b + 1) * (2 * epsilon) -epsilon  \n",
    "    return c"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-04T22:34:58.868525Z",
     "start_time": "2024-12-04T22:34:58.863428Z"
    }
   },
   "id": "6e4a5693c036b6b",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "The function performs feed-forward and backpropagation. \n",
    "\n",
    "Forward propagation: Input data is fed in the forward direction through the network. Each hidden layer accepts the input data, processes it as per the activation function and passes it to the successive layer. We will use the sigmoid function as our “activation function”.\n",
    "Backward propagation: It is the practice of fine-tuning the weights of a neural net based on the error rate obtained in the previous iteration.\n",
    "It also calculates cross-entropy costs for checking the errors between the prediction and original values. In the end, the gradient is calculated for the optimization objective.   "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5aca847eadffb249"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def neural_network(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lamb):\n",
    "    # Weights are split back to Theta1, Theta2\n",
    "    Theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)],\n",
    "                        (hidden_layer_size, input_layer_size + 1))\n",
    "    Theta2 = np.reshape(nn_params[hidden_layer_size * (input_layer_size + 1):], \n",
    "                        (num_labels, hidden_layer_size + 1))\n",
    "\n",
    "    # Forward propagation\n",
    "    m = X.shape[0]\n",
    "    one_matrix = np.ones((m, 1))\n",
    "    X = np.append(one_matrix, X, axis=1)  # Adding bias unit to first layer\n",
    "    a1 = X\n",
    "    z2 = np.dot(X, Theta1.transpose())\n",
    "    a2 = 1 / (1 + np.exp(-z2))  # Activation for second layer\n",
    "    one_matrix = np.ones((m, 1))\n",
    "    a2 = np.append(one_matrix, a2, axis=1)  # Adding bias unit to hidden layer\n",
    "    z3 = np.dot(a2, Theta2.transpose())\n",
    "    a3 = 1 / (1 + np.exp(-z3))  # Activation for third layer\n",
    "\n",
    "    # Changing the y labels into vectors of boolean values.\n",
    "    # For each label between 0 and 9, there will be a vector of length 10\n",
    "    # where the ith element will be 1 if the label equals i\n",
    "    y_vect = np.zeros((m, 10))\n",
    "    for i in range(m):\n",
    "        y_vect[i, int(y[i])] = 1\n",
    "\n",
    "    # Calculating cost function\n",
    "    J = (1 / m) * (np.sum(np.sum(-y_vect * np.log(a3) - (1 - y_vect) * np.log(1 - a3)))) + (lamb / (2 * m)) * (\n",
    "                sum(sum(pow(Theta1[:, 1:], 2))) + sum(sum(pow(Theta2[:, 1:], 2))))\n",
    "\n",
    "    # backprop\n",
    "    Delta3 = a3 - y_vect\n",
    "    Delta2 = np.dot(Delta3, Theta2) * a2 * (1 - a2)\n",
    "    Delta2 = Delta2[:, 1:]\n",
    "\n",
    "    # gradient\n",
    "    Theta1[:, 0] = 0\n",
    "    Theta1_grad = (1 / m) * np.dot(Delta2.transpose(), a1) + (lamb / m) * Theta1\n",
    "    Theta2[:, 0] = 0\n",
    "    Theta2_grad = (1 / m) * np.dot(Delta3.transpose(), a2) + (lamb / m) * Theta2\n",
    "    grad = np.concatenate((Theta1_grad.flatten(), Theta2_grad.flatten()))\n",
    "\n",
    "    return J, grad"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-04T22:34:58.907495Z",
     "start_time": "2024-12-04T22:34:58.898539Z"
    }
   },
   "id": "6dcca555a3561f3a",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "It performs forward propagation to predict the digit."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "59cf4f39ed645b5d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def predict(Theta1, Theta2, X):\n",
    "    m = X.shape[0]\n",
    "    one_matrix = np.ones((m, 1))\n",
    "    X = np.append(one_matrix, X, axis=1)  # Adding bias unit to first layer\n",
    "    z2 = np.dot(X, Theta1.transpose())\n",
    "    a2 = 1 / (1 + np.exp(-z2))  # Activation for second layer\n",
    "    one_matrix = np.ones((m, 1))\n",
    "    a2 = np.append(one_matrix, a2, axis=1)  # Adding bias unit to hidden layer\n",
    "    z3 = np.dot(a2, Theta2.transpose())\n",
    "    a3 = 1 / (1 + np.exp(-z3))  # Activation for third layer\n",
    "    p = (np.argmax(a3, axis=1))  # Predicting the class on the basis of max value of hypothesis\n",
    "    return p"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-04T22:34:58.950930Z",
     "start_time": "2024-12-04T22:34:58.945511Z"
    }
   },
   "id": "81cde4382e87c099",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "Importing all the required libraries, extract the data from mnist-original.mat file. Then features and labels will be separated from extracted data. After that data will be split into training (60,000) and testing (10,000) examples. Randomly initialize Thetas in the range of [-0.15, +0.15] to break symmetry and get better results. Further, the optimizer is called for the training of weights, to minimize the cost function for appropriate predictions. We have used the “minimize” optimizer from “scipy.optimize” library with “L-BFGS-B” method. We have calculated the test, the “training set accuracy and precision using “predict” function."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2e5eabbe2b82d96"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 97.330000\n",
      "Training Set Accuracy: 99.420000\n",
      "Precision = 0.9942\n"
     ]
    }
   ],
   "source": [
    "# Loading mat file\n",
    "data = loadmat('mnist-original.mat')\n",
    "\n",
    "# Extracting features from mat file\n",
    "X = data['data']\n",
    "X = X.transpose()\n",
    "\n",
    "# Normalizing the data\n",
    "X = X / 255\n",
    "\n",
    "# Extracting labels from mat file\n",
    "y = data['label']\n",
    "y = y.flatten()\n",
    "\n",
    "# Splitting data into training set with 60,000 examples\n",
    "X_train = X[:60000, :]\n",
    "y_train = y[:60000]\n",
    "\n",
    "# Splitting data into testing set with 10,000 examples\n",
    "X_test = X[60000:, :]\n",
    "y_test = y[60000:]\n",
    "\n",
    "m = X.shape[0]\n",
    "input_layer_size = 784  # Images are of (28 X 28) px so there will be 784 features\n",
    "hidden_layer_size = 100\n",
    "num_labels = 10  # There are 10 classes [0, 9]\n",
    "\n",
    "# Randomly initialising Thetas\n",
    "initial_Theta1 = initialise(hidden_layer_size, input_layer_size)\n",
    "initial_Theta2 = initialise(num_labels, hidden_layer_size)\n",
    "\n",
    "# Unrolling parameters into a single column vector\n",
    "initial_nn_params = np.concatenate((initial_Theta1.flatten(), initial_Theta2.flatten()))\n",
    "maxiter = 100\n",
    "lambda_reg = 0.1  # To avoid overfitting\n",
    "myargs = (input_layer_size, hidden_layer_size, num_labels, X_train, y_train, lambda_reg)\n",
    "\n",
    "# Calling minimize function to minimize cost function and to train weights\n",
    "results = minimize(neural_network, x0=initial_nn_params, args=myargs, \n",
    "          options={'disp': True, 'maxiter': maxiter}, method=\"L-BFGS-B\", jac=True)\n",
    "\n",
    "nn_params = results[\"x\"]  # Trained Theta is extracted\n",
    "\n",
    "# Weights are split back to Theta1, Theta2\n",
    "Theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)], (\n",
    "                              hidden_layer_size, input_layer_size + 1))  # shape = (100, 785)\n",
    "Theta2 = np.reshape(nn_params[hidden_layer_size * (input_layer_size + 1):], \n",
    "                      (num_labels, hidden_layer_size + 1))  # shape = (10, 101)\n",
    "\n",
    "# Checking test set accuracy of our model\n",
    "pred = predict(Theta1, Theta2, X_test)\n",
    "print('Test Set Accuracy: {:f}'.format((np.mean(pred == y_test) * 100)))\n",
    "\n",
    "# Checking train set accuracy of our model\n",
    "pred = predict(Theta1, Theta2, X_train)\n",
    "print('Training Set Accuracy: {:f}'.format((np.mean(pred == y_train) * 100)))\n",
    "\n",
    "# Evaluating precision of our model\n",
    "true_positive = 0\n",
    "for i in range(len(pred)):\n",
    "    if pred[i] == y_train[i]:\n",
    "        true_positive += 1\n",
    "false_positive = len(y_train) - true_positive\n",
    "print('Precision =', true_positive/(true_positive + false_positive))\n",
    "\n",
    "# Saving Thetas in .txt file\n",
    "np.savetxt('Theta1.txt', Theta1, delimiter=' ')\n",
    "np.savetxt('Theta2.txt', Theta2, delimiter=' ')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-04T22:37:12.215214Z",
     "start_time": "2024-12-04T22:34:58.993459Z"
    }
   },
   "id": "496ce705a1f5c89a",
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
